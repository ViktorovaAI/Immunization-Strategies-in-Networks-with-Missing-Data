{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rm\n",
    "import copy as cp\n",
    "import scipy.stats as stats\n",
    "import csv\n",
    "from spreading_CR import SpreadingProcess\n",
    "from zipfile import ZipFile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "Gamma = 1.0\n",
    "Beta = 0.95\n",
    "FracImNodes = 0.1\n",
    "max_cplusplus_int = 4294967295"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for evaluating centrality measures\n",
    "def list_meas_centr(Graph):\n",
    "    dgr = nx.degree_centrality(Graph)\n",
    "    btw = nx.betweenness_centrality(Graph, normalized=False)\n",
    "    ev = nx.eigenvector_centrality(Graph)\n",
    "    pr = nx.pagerank(Graph, alpha=0.9)\n",
    "    df1 = pd.DataFrame.from_dict({\n",
    "    'node': list(dgr.keys()),\n",
    "    'Dgr': list(dgr.values()),\n",
    "    'Btw': list(btw.values()),\n",
    "    'Ev': list(ev.values()),\n",
    "    'Pr': list(pr.values())\n",
    "    })\n",
    "    df1 = df1.round(6)\n",
    "    return(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function of creating a network from a file\n",
    "def create_netw(FileZip,i):\n",
    "    fl1 = FileZip.open('nodes ' + str(1) +'.csv')\n",
    "    fl2 = FileZip.open('edges ' + str(i) +'.csv')\n",
    "    fd1 = pd.read_csv(fl1, delimiter=\",\")\n",
    "    fd2 = pd.read_csv(fl2, delimiter=\",\", usecols=[1,2])\n",
    "    # Create network\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(fd1['0'])\n",
    "    G.add_edges_from(fd2[['0','1']].values.tolist())\n",
    "    return(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function of creating a data frame with centrality measures values\n",
    "def meas_list(FileZip,path):\n",
    "    fl = FileZip.open(path)\n",
    "    fd = pd.read_csv(fl, delimiter=\",\", usecols=[1,2,3,4,5])\n",
    "    fd.columns = ['node','Dgr','Btw','Ev','Pr']\n",
    "    return(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for evaluating traditional robustness and list of nodes in top 10\n",
    "def nodes_list(df1,df2,x,NumbNod):\n",
    "    #Define the rank of centrality value\n",
    "    df1['rank'] = df1[x].rank(method='max')\n",
    "    df2['rank'] = df2[x].rank(method='max')\n",
    "    #Create data frame with node that exists in two frames\n",
    "    df3 = pd.merge(df2,df1,how='left',on='node')\n",
    "    #Compute the Kendall correlation\n",
    "    TauVal = stats.kendalltau(df2['rank'],df3['rank_y']).correlation\n",
    "    TauVal = TauVal.round(6)\n",
    "    #Sort data frame of obsevied network on centrality measures\n",
    "    df2 = df2.sort_values(x, ascending=False)   \n",
    "    #Define the first 10% nodes\n",
    "    srList = df2.head(n=NumbNod)\n",
    "    OvSrList = srList['node'].values.tolist()  \n",
    "    return(TauVal, OvSrList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function of creating a data frame with immunization nodes\n",
    "def list_nodes_im(FileZip,i,j,cm,G):\n",
    "    fl1 = FileZip.open('netw_' + str(i) +'_mis_'+ str(j) +'_'+ cm +'.csv')\n",
    "    CList = pd.read_csv(fl1, delimiter=\",\", usecols=[1])\n",
    "    CList.columns=['node']\n",
    "    Clist = list(G.edges(CList['node']))\n",
    "    return(Clist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for evaluating SIR model and outbreak size determination\n",
    "def sim_sir(Beta,Gamma,cList):\n",
    "    sp = SpreadingProcess(cList,Beta,Gamma,0)\n",
    "    seed = np.random.randint(max_cplusplus_int+1)\n",
    "    R0_mean, R0_std=sp.estimate_R0(2000,seed)\n",
    "    sp.reset()\n",
    "    R0_mean = np.round(R0_mean,4)\n",
    "    R0_std = np.round(R0_std,4)\n",
    "    return (R0_mean, R0_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload data from csv file\n",
    "fd = pd.read_csv(\"edgelist.truecolsprings.csv\", delimiter=\",\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "ColSp = nx.Graph()\n",
    "ColSp.add_edges_from(fd[['V1','V2']].values.tolist())\n",
    "# Define degree sequence from Colorado Spring network\n",
    "degree_sequence = [d for n, d in ColSp.degree()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draw Colorado Spring network\n",
    "nx.draw(ColSp,pos = nx.spring_layout(ColSp),node_size=60,font_size=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draw degree distribution from Colorado Spring network\n",
    "plt.hist(degree_sequence, density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generation and save the 5000 configuration models into zip-file\n",
    "newzip = ZipFile('true_netw.zip','w')\n",
    "# Create a true network\n",
    "for i in range(1,5001):\n",
    "    # File save paths\n",
    "    path_1 = 'nodes ' + str(i) +'.csv'\n",
    "    path_2 = 'edges ' + str(i) +'.csv'\n",
    "    seed=rm.seed()\n",
    "    # Create graph using configuration model\n",
    "    G = nx.configuration_model(degree_sequence,seed=seed)\n",
    "    #remove parallel edges\n",
    "    G = nx.Graph(G)\n",
    "    #remove self-loops\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "    # Save grapth in file\n",
    "    pd.DataFrame(G.edges).to_csv(path_2)\n",
    "    pd.DataFrame(G.nodes).to_csv(path_1)\n",
    "    newzip.write(path_1)\n",
    "    newzip.write(path_2)\n",
    "    os.remove(path_1)\n",
    "    os.remove(path_2)\n",
    "    \n",
    "newzip.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload files from zip-file and compute centrality measures for G true network\n",
    "FileZip = ZipFile('true_netw.zip','r')\n",
    "newzip = ZipFile('G_cm.zip','w')\n",
    "for i in range(1,5001):\n",
    "    G = create_netw(FileZip,i)\n",
    "    path_f = 'netw_' + str(i) +'.csv'\n",
    "    df2 = list_meas_centr(G)\n",
    "    df2.to_csv(path_f)\n",
    "    newzip.write(path_f)\n",
    "    os.remove(path_f)\n",
    "    \n",
    "newzip.close()\n",
    "FileZip.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generation a network with missing data and compute centrality measures. Identifying the random immunization list nodes\n",
    "FileZip = ZipFile('true_netw.zip','r')\n",
    "newzip = ZipFile('G_imstr.zip','w')\n",
    "for i in range(1,5001):\n",
    "    G = create_netw(FileZip1,i)\n",
    "    for j in range(1,9):\n",
    "        # Remove random nodes - ...%\n",
    "        num = int(len(G.nodes())*(j/10))\n",
    "        # Choose random nodes from true network\n",
    "        rm.seed()\n",
    "        rnodes = rm.sample(list(G.nodes()),num)\n",
    "        # Remove selected random nodes from copy of true network\n",
    "        G.remove_nodes_from(rnodes)\n",
    "        #Number of nodes to be immunized\n",
    "        numNo = int(len(G.nodes)*FracImNodes)\n",
    "        #Random immunization list nodes\n",
    "        rm.seed()\n",
    "        rdm = rm.sample(list(G.nodes),numNo)\n",
    "        path_r = 'netw_' + str(i) +'_mis_'+str(j) +'_rdm'+'.csv'\n",
    "        df2 = pd.DataFrame(rdm).to_csv(path_r)\n",
    "        newzip.write(path_r)\n",
    "        os.remove(path_r)\n",
    "        # Immunization strategy measures\n",
    "        df1 = list_meas_centr(G)\n",
    "        df1.to_csv(path_f)\n",
    "        path_f = 'netw_' + str(i) +'_mis_'+str(j) +'.csv'\n",
    "        newzip.write(path_f)\n",
    "        os.remove(path_f)\n",
    "        \n",
    "newzip.close()\n",
    "FileZip.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value of Robustness\n",
    "R=[]\n",
    "Rim = []\n",
    "Rim_std =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculation of traditional robustness and determination of the list of nodes of the immunization strategy\n",
    "FileZip1 = ZipFile('G_cm.zip','r')\n",
    "FileZip2 = ZipFile('G_imstr.zip','r')\n",
    "newzip = ZipFile('list_im.zip','w')\n",
    "for j in range(1,9):\n",
    "    for i in range(1,5001):\n",
    "        path = 'netw_' + str(i) +'.csv'\n",
    "        #Centrality measures of true network\n",
    "        fd1 = meas_list(FileZip1,path)\n",
    "        path = 'netw_' + str(i) +'_mis_' + str(j) +'.csv'\n",
    "        #Centrality measures of netwotk with missing data\n",
    "        fd2 = meas_list(FileZip2, path)\n",
    "        #number of nodes in the top 10\n",
    "        numNo = int(len(fd2)*FracImNodes)\n",
    "        #Definition of robustness and list of nodes for each immunization strategy\n",
    "        Dgr, Dlist = nodes_list(fd1,fd2,'Dgr',numNo)\n",
    "        Btw, Blist = nodes_list(fd1,fd2,'Btw',numNo)\n",
    "        Ev, Elist = nodes_list(fd1,fd2,'Ev',numNo)\n",
    "        Pr, Plist = nodes_list(fd1,fd2,'Pr',numNo)\n",
    "        #Save the results for each level of missing data\n",
    "        R.append([Dgr,Btw,Ev,Pr,j*10])\n",
    "        #Save the list of nodes for each immunization strategy (top 10)\n",
    "        path_d = 'netw_' + str(i) +'_mis_'+str(j) +'_d'+'.csv'\n",
    "        pd.DataFrame(Dlist).to_csv(path_d)\n",
    "        path_b = 'netw_' + str(i) +'_mis_'+str(j) +'_b'+'.csv'\n",
    "        pd.DataFrame(Blist).to_csv(path_b)\n",
    "        path_e = 'netw_' + str(i) +'_mis_'+str(j) +'_e'+'.csv'\n",
    "        pd.DataFrame(Elist).to_csv(path_e)       \n",
    "        path_p = 'netw_' + str(i) +'_mis_'+str(j) +'_p'+'.csv'\n",
    "        pd.DataFrame(Plist).to_csv(path_p)       \n",
    "        newzip.write(path_d)\n",
    "        newzip.write(path_b)\n",
    "        newzip.write(path_e)\n",
    "        newzip.write(path_p)\n",
    "        os.remove(path_d)\n",
    "        os.remove(path_b)\n",
    "        os.remove(path_e)\n",
    "        os.remove(path_p)\n",
    "\n",
    "newzip.close()\n",
    "FileZip1.close()      \n",
    "FileZip2.close()\n",
    "path_r = 'Robust_tr.csv'\n",
    "pd.DataFrame(R).to_csv(path_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rim =[]\n",
    "Rim_std = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SIR model calculation and outbreak size determination for each immunization strategy\n",
    "FileZip1 = ZipFile('true_netw.zip','r')\n",
    "FileZip2 = ZipFile('list_im_rdm.zip','r')\n",
    "for i in range(2501,3001):\n",
    "    G = create_netw(FileZip1,i)\n",
    "    for j in range(1,9):\n",
    "        #Upload list of nodes\n",
    "        dgrList = list_nodes_im(FileZip2,i,j,'d',G)\n",
    "        btwList = list_nodes_im(FileZip2,i,j,'b',G)\n",
    "        evList = list_nodes_im(FileZip2,i,j,'e',G)\n",
    "        prList = list_nodes_im(FileZip2,i,j,'p',G)\n",
    "        rdmList = list_nodes_im(FileZip2,i,j,'rdm',G)\n",
    "        #Outbreak size determination\n",
    "        R1,R1_std = sim_sir(Beta,Gamma,rdmList)\n",
    "        R2, R2_std = sim_sir(Beta, Gamma,dgrList)\n",
    "        R3, R3_std = sim_sir(Beta, Gamma, btwList)\n",
    "        R4, R4_std = sim_sir(Beta, Gamma, evList)\n",
    "        R5, R5_std = sim_sir(Beta, Gamma,prList)\n",
    "        #Save the results for each level of missing data\n",
    "        Rim.append([R1,R2,R3,R4,R5,j*10])\n",
    "        Rim_std.append([R1_std,R2_std,R3_std,R4_std,R5_std,j*10])\n",
    "\n",
    "\n",
    "FileZip1.close()\n",
    "FileZip2.close()\n",
    "path_r = 'Robust_im.csv'\n",
    "pd.DataFrame(Rim).to_csv(path_r)\n",
    "path_r = 'Robust_im_std.csv'\n",
    "pd.DataFrame(Rim_std).to_csv(path_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimation of the mean and standard deviation for traditional robustness\n",
    "df1 = pd.read_csv('Robust_tr.csv',delimiter = \";\", usecols=[1,2,3,4,5])\n",
    "df1.columns=['Dgr','Btw','Eig','PR','p%']\n",
    "MV_R = df1.groupby(['p%']).mean()\n",
    "Std_R = df1.groupby(['p%']).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimation of the mean and standard deviation in case of immunization strategies\n",
    "df2 = pd.read_csv('Robust_im.csv', delimiter = \",\", usecols=[1,2,3,4,5,6])\n",
    "df2.columns=['Rdm','Dgr','Btw','Eig','PR','p%']\n",
    "df2['Dgr-Rdm'] = df2['Dgr']-df2['Rdm']\n",
    "df2['Btw-Rdm'] = df2['Btw']-df2['Rdm']\n",
    "df2['Eig-Rdm'] = df2['Eig']-df2['Rdm']\n",
    "df2['PR-Rdm'] = df2['PR']-df2['Rdm']\n",
    "MV_Rim = df2.groupby(['p%']).mean()\n",
    "Std_Rim = df2.groupby(['p%']).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of results\n",
    "# Degree centrality\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.errorbar(MV_R.index, MV_Rim['Dgr-Rdm'], yerr= Std_Rim['Dgr-Rdm'],\n",
    "             fmt='-o', ecolor='blue', c='blue', ms=3)\n",
    "ax1.set_ylabel('delta in Outbreak size vs. Random immunization')\n",
    "ax1.set_xlabel('Percent of nodes missing for sample')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.errorbar(MV_R.index, MV_R['Dgr'], yerr= Std_R['Dgr'],\n",
    "             fmt='-o', ecolor='red', c='red', ms=3)\n",
    "ax2.set_ylabel('Robustness of centrality')\n",
    "plt.title('Degree centrality')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Betweenness centrality\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.errorbar(MV_R.index, MV_Rim['Btw-Rdm'], yerr= Std_Rim['Btw-Rdm'],\n",
    "             fmt='-o', ecolor='blue', c='blue', ms=3)\n",
    "ax1.set_ylabel('delta in Outbreak size vs. Random immunization')\n",
    "ax1.set_xlabel('Percent of nodes missing for sample')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.errorbar(MV_R.index, MV_R['Btw'], yerr= Std_R['Btw'],\n",
    "             fmt='-o', ecolor='red', c='red', ms=3)\n",
    "ax2.set_ylabel('Robustness of centrality')\n",
    "plt.title('Betweenness centrality')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Eigenvector centrality\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.errorbar(MV_R.index, MV_Rim['Eig-Rdm'], yerr= Std_Rim['Eig-Rdm'],\n",
    "             fmt='-o', ecolor='blue', c='blue', ms=3)\n",
    "ax1.set_ylabel('delta in Outbreak size vs. Random immunization')\n",
    "ax1.set_xlabel('Percent of nodes missing for sample')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.errorbar(MV_R.index, MV_R['Eig'], yerr= Std_R['Eig'],\n",
    "             fmt='-o', ecolor='red', c='red', ms=3)\n",
    "ax2.set_ylabel('Robustness of centrality')\n",
    "plt.title('Eigenvector centrality')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pagerank centrality\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.errorbar(MV_R.index, MV_Rim['PR-Rdm'], yerr= Std_Rim['PR-Rdm'],\n",
    "             fmt='-o', ecolor='blue', c='blue', ms=3)\n",
    "ax1.set_ylabel('delta in Outbreak size vs. Random immunization')\n",
    "ax1.set_xlabel('Percent of nodes missing for sample')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.errorbar(MV_R.index, MV_R['PR'], yerr= Std_R['PR'],\n",
    "             fmt='-o', ecolor='red', c='red', ms=3)\n",
    "ax2.set_ylabel('Robustness of centrality')\n",
    "plt.title('Pagerank centrality')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
